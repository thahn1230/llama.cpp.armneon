#!/bin/bash

echo "ğŸ” === Q4_0 Weight Activation Debug Test ==="

# 1. ë¹Œë“œ í™•ì¸
if [ ! -f "./llama-cli" ]; then
    echo "ğŸ“¦ Building llama.cpp with debug output..."
    make clean
    make -j$(nproc) CFLAGS="-DGGML_USE_LLAMAFILE=1 -O2 -g" CXXFLAGS="-DGGML_USE_LLAMAFILE=1 -O2 -g"
fi

# 2. í…ŒìŠ¤íŠ¸ ëª¨ë¸ í™•ì¸
TEST_MODEL="models/test_q4_0.gguf"
if [ ! -f "$TEST_MODEL" ]; then
    echo "âŒ Q4_0 í…ŒìŠ¤íŠ¸ ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
    echo "   ì‚¬ìš©ë²•:"
    echo "   1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ: huggingface-cli download microsoft/DialoGPT-small ggml-model-f16.gguf"
    echo "   2. Q4_0 ë³€í™˜: ./llama-quantize ggml-model-f16.gguf $TEST_MODEL Q4_0"
    exit 1
fi

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export GGML_USE_LLAMAFILE=1
export GGML_DEBUG=1

# 4. Q4_0 activation ë””ë²„ê·¸ í…ŒìŠ¤íŠ¸
echo ""
echo "ğŸš€ Testing Q4_0 weight with activation quantization debug..."
echo "   Model: $TEST_MODEL"
echo "   Input: 'Hello'"
echo ""

# ë””ë²„ê·¸ ì¶œë ¥ì„ ìº¡ì²˜í•˜ê¸° ìœ„í•´ íŒŒì¼ë¡œ ì €ì¥
./llama-cli -m "$TEST_MODEL" -p "Hello" -n 1 --temp 0 2>&1 | tee debug_output.log

echo ""
echo "ğŸ“Š === Debug Output Analysis ==="
echo ""

# ì¤‘ìš”í•œ ë””ë²„ê·¸ ì •ë³´ ì¶”ì¶œ
echo "ğŸ” Q4_0 Weight Processing:"
grep -A 10 "Q4_0 WEIGHT PROCESSING DEBUG" debug_output.log || echo "   No Q4_0 processing found"

echo ""
echo "ğŸ“Š F32 â†’ Q8_0 Quantization:"
grep -A 15 "F32 â†’ Q8_0 QUANTIZATION PROCESS" debug_output.log || echo "   No quantization debug found"

echo ""
echo "ğŸš€ SGEMM Kernel Usage:"
grep -A 10 "Q4_0 WEIGHT DEBUG" debug_output.log || echo "   No SGEMM debug found"

echo ""
echo "ğŸ“ Full debug log saved to: debug_output.log"
echo "   Use 'cat debug_output.log' to see complete output"

echo ""
echo "âœ… Q4_0 activation debug test completed!" 