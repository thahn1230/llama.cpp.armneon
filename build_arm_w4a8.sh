#!/bin/bash

# ARM W4A8 ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
echo "ğŸš€ Building llama.cpp with W4A8 support for ARM..."

# ê¸°ì¡´ ë¹Œë“œ ë””ë ‰í† ë¦¬ ì •ë¦¬
rm -rf build_arm
mkdir -p build_arm
cd build_arm

# ARM W4A8 ìµœì í™” ë¹Œë“œ ì„¤ì •
cmake .. \
    -DCMAKE_BUILD_TYPE=Release \
    -DGGML_LLAMAFILE=ON \
    -DGGML_CPU_KLEIDIAI=ON \
    -DGGML_NATIVE=ON \
    -DGGML_CPU=ON \
    -DGGML_BACKEND_DL=OFF \
    -DCMAKE_C_FLAGS="-march=armv8-a+dotprod+i8mm -mtune=cortex-a76 -O3" \
    -DCMAKE_CXX_FLAGS="-march=armv8-a+dotprod+i8mm -mtune=cortex-a76 -O3"

echo "ğŸ“‹ Configuration completed. Building..."

# ë³‘ë ¬ ë¹Œë“œ
make -j$(nproc)

echo "âœ… Build completed!"
echo ""
echo "ğŸ” Testing W4A8 activation:"
echo "  ./bin/llama-cli -m your_model_Q4_0.gguf -p \"Hello\" -n 5"
echo ""
echo "ğŸ“Š Expected output should show:"
echo "  ğŸ¯ W4A8 KERNEL: ggml_vec_dot_q4_0_q8_0 called!" 