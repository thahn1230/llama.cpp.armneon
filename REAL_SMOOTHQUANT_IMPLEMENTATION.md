# ğŸ”¥ ì§„ì§œ SmoothQuant W8A8 êµ¬í˜„ ê³„íš

## âŒ **í˜„ì¬ ë¬¸ì œì **
- í˜„ì¬ Q8_A8ëŠ” **ê°€ì¤‘ì¹˜ë§Œ 8-bit, í™œì„±í™”ëŠ” float32** (W8A16)
- SmoothQuant ìŠ¤ì¼€ì¼ë§ì€ ì ìš©ë˜ì—ˆì§€ë§Œ **ëŸ°íƒ€ì„ activation quantization ì—†ìŒ**

## âœ… **ì‹¤ì œ SmoothQuant ìš”êµ¬ì‚¬í•­**

### 1ï¸âƒ£ **êµ¬ì¡°ì²´ ì¬ì •ì˜**
```c
typedef struct {
    ggml_half weight_scale;      // ê°€ì¤‘ì¹˜ ì–‘ìí™” ìŠ¤ì¼€ì¼
    ggml_half activation_scale;  // í™œì„±í™” ì–‘ìí™” ìŠ¤ì¼€ì¼ (ëŸ°íƒ€ì„ ê³„ì‚°)
    int8_t weight_qs[QK8_A8];    // 8-bit ì–‘ìí™”ëœ ê°€ì¤‘ì¹˜
    // í™œì„±í™”ëŠ” ëŸ°íƒ€ì„ì— ë™ì  ì–‘ìí™”
} block_q8_a8_v2;
```

### 2ï¸âƒ£ **ëŸ°íƒ€ì„ Activation Quantization**
```c
// Forward passì—ì„œ í™œì„±í™”ë¥¼ ë™ì ìœ¼ë¡œ 8-bit ì–‘ìí™”
void quantize_activation_q8_a8(const float* input, int8_t* output, 
                               ggml_half* scale, int size) {
    // í™œì„±í™”ì˜ ì ˆëŒ€ê°’ ìµœëŒ€ì¹˜ ì°¾ê¸°
    float amax = 0.0f;
    for (int i = 0; i < size; i++) {
        amax = fmaxf(amax, fabsf(input[i]));
    }
    
    // 8-bit ì–‘ìí™” ìŠ¤ì¼€ì¼ ê³„ì‚°
    *scale = amax / 127.0f;
    float inv_scale = (*scale != 0) ? 127.0f / amax : 0.0f;
    
    // í™œì„±í™”ë¥¼ 8-bitë¡œ ì–‘ìí™”
    for (int i = 0; i < size; i++) {
        output[i] = (int8_t)roundf(input[i] * inv_scale);
    }
}
```

### 3ï¸âƒ£ **W8A8 Matrix Multiplication**
```c
// 8-bit weights Ã— 8-bit activations
void ggml_vec_dot_q8_a8_q8_a8_real(int n, float* s, 
                                   const void* vx, const void* vy) {
    const block_q8_a8_v2* x = vx;
    const block_q8_a8_v2* y = vy;  // yëŠ” ëŸ°íƒ€ì„ì— ì–‘ìí™”ëœ í™œì„±í™”
    
    float sumf = 0.0f;
    const int nb = n / QK8_A8;
    
    for (int i = 0; i < nb; i++) {
        int32_t sumi = 0;
        
        // 8-bit weights Ã— 8-bit activations
        for (int j = 0; j < QK8_A8; j++) {
            sumi += (int32_t)x[i].weight_qs[j] * (int32_t)y[i].weight_qs[j];
        }
        
        // ìŠ¤ì¼€ì¼ ì ìš©: weight_scale Ã— activation_scale
        float scale = GGML_FP16_TO_FP32(x[i].weight_scale) * 
                     GGML_FP16_TO_FP32(y[i].activation_scale);
        sumf += sumi * scale;
    }
    
    *s = sumf;
}
```

## ğŸš€ **êµ¬í˜„ ë‹¨ê³„**

### Step 1: Forward Pass Hook ì¶”ê°€
```c
// ggml-cpu.cì˜ forward passì—ì„œ activationì„ ë™ì  ì–‘ìí™”
static void ggml_compute_forward_mul_mat_q8_a8_real(
    const struct ggml_compute_params* params,
    struct ggml_tensor* dst) {
    
    // í™œì„±í™” í…ì„œë¥¼ ëŸ°íƒ€ì„ì— 8-bitë¡œ ì–‘ìí™”
    quantize_activation_runtime(src1, quantized_activation);
    
    // W8A8 ì—°ì‚° ìˆ˜í–‰
    ggml_vec_dot_q8_a8_q8_a8_real(...);
}
```

### Step 2: SmoothQuant Policy ì ìš©
```python
# Pythonì—ì„œ ë” ì •êµí•œ SmoothQuant ì ìš©
def apply_smoothquant_policy(model, alpha=0.5):
    for layer_name, layer in model.named_modules():
        if isinstance(layer, nn.Linear):
            # í™œì„±í™” í†µê³„ ìˆ˜ì§‘
            activation_stats = collect_activation_stats(layer)
            
            # SmoothQuant ìŠ¤ì¼€ì¼ë§
            s = activation_stats.max(dim=-1, keepdim=True)[0].clamp(min=1e-5)
            s = s ** alpha / (layer.weight.abs().max(dim=0, keepdim=True)[0] ** (1-alpha))
            
            # ê°€ì¤‘ì¹˜ì™€ í™œì„±í™”ì— ì—­ ìŠ¤ì¼€ì¼ë§ ì ìš©
            layer.weight.data = layer.weight.data * s
            # ë‹¤ìŒ ë ˆì´ì–´ ì…ë ¥ì— 1/s ìŠ¤ì¼€ì¼ë§ ì ìš©
            apply_inverse_scaling_to_next_layer(s)
```

## ğŸ“± **ì•ˆë“œë¡œì´ë“œ ARM ìµœì í™”**

### ARM NEON W8A8 ìµœì í™”
```c
#if defined(__ARM_NEON)
void ggml_vec_dot_q8_a8_q8_a8_neon(int n, float* s, 
                                   const void* vx, const void* vy) {
    // ARM MATMUL_INT8 instruction ì‚¬ìš©
    // 8x8 â†’ 32-bit accumulation
    // ë²¡í„°í™”ëœ 8-bit Ã— 8-bit ì—°ì‚°
}
#endif
```

## ğŸ¯ **ê¸°ëŒ€ íš¨ê³¼**

1. **ë©”ëª¨ë¦¬**: 50% ì ˆì•½ (W8A8 vs FP16)
2. **ì†ë„**: ARMì—ì„œ 2-3x í–¥ìƒ (MATMUL_INT8)
3. **ì •í™•ë„**: SmoothQuantë¡œ quantization error ìµœì†Œí™”
4. **ì‹¤ìš©ì„±**: 8GB RAM ì•ˆë“œë¡œì´ë“œì—ì„œ 7B ëª¨ë¸ ì‹¤í–‰

## âš ï¸ **ì£¼ì˜ì‚¬í•­**

- **ë™ì  ì–‘ìí™” ì˜¤ë²„í—¤ë“œ**: Forward passë§ˆë‹¤ activation quantization í•„ìš”
- **ë©”ëª¨ë¦¬ ì¦ê°€**: ëŸ°íƒ€ì„ì— temporary quantized activation buffer í•„ìš”
- **ì •í™•ë„ trade-off**: W8A8ëŠ” W8A16ë³´ë‹¤ ì•½ê°„ì˜ ì •í™•ë„ ì†ì‹¤ 